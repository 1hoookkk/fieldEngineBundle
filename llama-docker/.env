########################################
# llama.cpp server configuration
########################################

# Host side port -> container port
HOST_PORT=8080
SERVER_PORT=8080

# Where your GGUF models live on the host (Windows default)
HOST_MODEL_DIR=C:/downloads
# For Linux/WSL users you can override, e.g.:
# HOST_MODEL_DIR=/downloads

# Optional: explicitly set the model path inside container
# If empty, the entrypoint will auto-pick the first GGUF in /models
# MODEL_PATH=/models/qwen3-1.5b-instruct-q4_k_m.gguf

# Performance tuning
CTX_SIZE=4096
THREADS=0     # 0 = auto (all CPUs)
NGL=35        # GPU layers for CUDA; set 0 for CPU-only

# Extra flags for server (e.g., --embedding --chat-template <file>)
SERVER_EXTRA_ARGS=

# Build selection: cpu or cuda
BUILD_TARGET=cuda

# GPU access for compose (requires NVIDIA Container Toolkit)
GPUS=all

