version: "3.8"

services:
  llama:
    build:
      context: .
      target: ${BUILD_TARGET:-cuda}
    container_name: llama-cpp-server
    ports:
      - "${HOST_PORT:-8080}:${SERVER_PORT:-8080}"
    environment:
      - MODEL_PATH=${MODEL_PATH}
      - SERVER_PORT=${SERVER_PORT:-8080}
      - CTX_SIZE=${CTX_SIZE:-4096}
      - THREADS=${THREADS:-0}
      - NGL=${NGL:-35}
      - SERVER_EXTRA_ARGS=${SERVER_EXTRA_ARGS:-}
    volumes:
      - ${HOST_MODEL_DIR:-C:/downloads}:/models:ro
    # Enable GPU access when using CUDA target
    gpus: ${GPUS:-all}
    restart: unless-stopped

